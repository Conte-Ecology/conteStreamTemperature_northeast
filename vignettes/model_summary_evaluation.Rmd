---
title: "Model Summary and Validation"
author: "Daniel J Hocking"
date: "11/9/2015"
output: html_document
---


Load packages

```{r}
library(dplyr)
library(conteStreamTemperature)
```

set custom themes

```{r presentation ggplot theme, echo=FALSE}
theme_bw_journal <- function (base_family = "") {
    theme_grey(base_family = base_family) %+replace% 
        theme(axis.text = element_text(size = rel(0.8)), axis.ticks = element_line(colour = "black"), 
            legend.key = element_rect(colour = "grey80"), panel.background = element_rect(fill = "white", 
                colour = NA), panel.border = element_rect(fill = NA, 
                colour = "grey50"), panel.grid.major = element_line(colour = "grey90", 
                size = 0.2), panel.grid.minor = element_line(colour = "grey98", 
                size = 0.5), strip.background = element_rect(fill = "grey80", 
                colour = "grey50", size = 0.2))
    }

theme_set(theme_bw_journal())
```

load data

```{r}
data_dir <- "localData_2016-02-26_newDelineation/localData_2016-02-26_newDelineation" 

load(paste0(data_dir, "/tempDataSync.RData"))

M.ar1 <- readRDS(paste0(data_dir, "/jags.RData"))

cov.list <- readRDS(paste0(data_dir, "/covariate_list.RData"))

coef.list <- readRDS(paste0(data_dir, "/coef.RData"))

derived.metrics <- readRDS(paste0(data_dir, "/derived_site_metrics.RData"))

breakpoints <- readRDS(paste0(data_dir, "/springFallBPs.RData"))

derived.metrics.obs <- readRDS(paste0(data_dir, "/derived_site_metrics_observed.RData"))

```


### Coefficients

Summarize coefficients and make sure they're reasonable

```{r}
format(coef.list$fix.ef, digits = 2)

format(coef.list$mu.huc, digits = 2)

format(coef.list$sigma.site, digits = 2)

format(coef.list$sigma.huc, digits = 2)

format(coef.list$sigma.year, digits = 2)

format(coef.list$sigma.ar1, digits = 2)

format(coef.list$cor.huc, digits = 2)

format(coef.list$cor.year, digits = 2)

```

Surprisingly no effect of forest cover, but it might get sucked up by the previous 7-day air temperature. It's also possible that there just isn't sufficient power to have forest, agriculture, and hi-development plus interactions and all the random effects in the model.

There is large variablity among sites and hucs. Little variation of AR1 among sites.


### Check RMSE predictions in R

```{r}
# Predictions in R
tempDataSyncS <- predictTemp(data = tempDataSyncS, coef.list = coef.list, cov.list = cov.list, rand_ids = rand_ids, Random_AR1 = FALSE)

rmse(tempDataSyncS$temp - tempDataSyncS$tempPredicted) # 0.6398
  
```

### Check predictions in JAGS

```{r}
df.ar1 <- as.data.frame(as.matrix(M.ar1)) # massive

# predictions from JAGS
temp.predicted.mean <- df.ar1 %>%
  dplyr::select(starts_with("stream.mu")) %>%
  colMeans()
tempDataSyncS$tempPredictedJags <- as.numeric(temp.predicted.mean)

ggplot(tempDataSyncS, aes(temp, tempPredictedJags)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$temp - tempDataSyncS$tempPredictedJags) # 0.633 - slightly diff than R predictions

```


### JAGS trend predictions

```{r}
# predictions from JAGS
temp.trend.mean <- df.ar1 %>%
  dplyr::select(starts_with("trend")) %>%
  colMeans()
tempDataSyncS$tempTrendJags <- NA_real_
tempDataSyncS[1:length(temp.trend.mean), ]$tempTrendJags <- as.numeric(temp.trend.mean)

ggplot(tempDataSyncS, aes(temp, tempTrendJags)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$temp - tempDataSyncS$tempTrendJags) # 1.15
rmse(tempDataSyncS$temp - tempDataSyncS$trend) # 1.19

```

### Check That Prediction Function Matches JAGS Predictions

```{r}
ggplot(tempDataSyncS, aes(tempPredictedJags, tempPredicted)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$tempPredictedJags - tempDataSyncS$tempPredicted)
```

The predictions are slightly different when calculated in JAGS vs. in R. I think the difference is in calculating them for each iteration then taking the mean (in JAGS) compared with taking the mean coefficients then doing the predictions. 


### Test Predictions and RMSE for observed and validation data

```{r}
rmse(tempDataSyncS$temp - tempDataSyncS$tempPredicted)
rmse(tempDataSyncS$temp - tempDataSyncS$tempPredictedJags)
rmse(tempDataSyncS$temp - tempDataSyncS$trend) 

tempDataSyncValidS <- predictTemp(data = tempDataSyncValidS, coef.list = coef.list, cov.list = cov.list, rand_ids = rand_ids, Random_AR1 = FALSE)
rmse(tempDataSyncValidS$temp - tempDataSyncValidS$tempPredicted) 
rmse(tempDataSyncValidS$temp - tempDataSyncValidS$trend) 
```

Very disappointing validation. Potential problems:

* difficult validation holding out sites, hucs, and years
* Correlation of airTemp and 7-day air temp with the cubic day of the year
* don't adequately address things like groundwater, dams, and tidal influence
* problem with one or more of the functions
* Imprecise covariates
* Not adequate QAQC on temperature data
* model is overfitted, especially with all the random site, huc, and year effects. Overfitted models do well with fitting the data but poorly at predicting new data.


### Plot observed, predicted, and air temperatures

```{r}
foo <- dplyr::select(tempDataSyncS, featureid, date, fixed.ef, site.ef, huc.ef, year.ef, trend, tempPredicted, tempPredictedJags, tempTrendJags)
tempDataSync <- tempDataSync %>%
  dplyr::left_join(foo)
tempDataSync$validation <- "calibrate"

foo <- dplyr::select(tempDataSyncValidS, featureid, date, fixed.ef, site.ef, huc.ef, year.ef, trend, tempPredicted)
tempDataSyncValid <- tempDataSyncValid %>%
  dplyr::left_join(foo)
tempDataSyncValid$validation <- "validate"

master_data <- dplyr::rbind_list(tempDataSync, tempDataSyncValid)

master_data <- master_data %>%
  flag_daily_rise(.)

# Median Absolute Deviations for spike detection
master_data <- master_data %>%
  dplyr::group_by(featureid, year) %>%
  dplyr::mutate(MAD_normalized = MAD.roller(temp, 6),
                MAD_normalized_10 = MAD.roller(temp, 10),
                MAD_normalized_30 = MAD.roller(temp, 30),
                flag_MAD = ifelse(MAD_normalized > 10, TRUE, FALSE))

master_data$id_year <- paste0(master_data$featureid, "_", master_data$year)

rmse_sum <- master_data %>%
  group_by(id_year) %>%
  mutate(RMSE = rmse(temp - trend)) %>%
  summarise(RMSE = mean(RMSE, na.rm = T))

master_data <- dplyr::left_join(master_data, rmse_sum)

series <- unique(master_data$id_year)
n <- length(series)
for(i in 1:n) {
  data_1 <- dplyr::filter(master_data, id_year == series[i])
  g <- ggplot(data_1, aes(date, temp)) + ylim(c(0, 35)) + ggtitle(paste0(data_1$validation, " |  featureid: ", data_1$featureid[1], " | year: ", data_1$year[1], " | RMSE = ", round(data_1$RMSE, digits = 2))) #+ facet_wrap(~year)
#   if(nrow(bad_MAD_30) > 0) {
#     g <- g + geom_point(data = bad_MAD_30, aes(datetime, temp), colour = "yellow", size = 5, alpha = 0.5)
#   }
  g <- g + geom_point(aes(colour = MAD_normalized_10)) + geom_line(aes(colour = MAD_normalized_10)) + 
    scale_colour_gradient(high = "yellow", low = "blue", limits = c(0, 6)) 
  
  # add agency name
  g <- g +
    geom_line(aes(date, airTemp), colour = "black") +
    #geom_line(aes(date, tmax), colour = "gray") +
    #geom_line(aes(date, tmin), colour = "gray") +
    geom_line(aes(date, trend), colour = "red") + 
    xlab("Date") + 
    ylab("Temperature (C)") +
    theme_bw()
  ggsave(paste0(data_dir, "/diagnostics/obs_pred_ts_", series[i], ".png"), plot = g)
}



# output daily flags
# d_flags <- df_values3 %>%
#   dplyr::filter(flag_daily_rise == TRUE |
#                   flag_cold_days == TRUE |
#                   flag_hot_days == TRUE |
#                   flag_extreme_days == TRUE) %>%
#   dplyr::select(-row, -temp_prev, -series_prev, -series_start, -median_freq, -min_n90)

# Median Absolute Deviations for spike detection



```




### Is there a trade-off with AR1 term by site and other random site effects?

```{r, echo=FALSE}
Random_AR1 <- FALSE
if(Random_AR1) {
df_site_effects <- coef.list$B.site %>%
  dplyr::left_join(dplyr::select(coef.list$B.ar1, sitef, site, B.ar1 = mean)) %>%
  dplyr::rename(B.site_intercept = B.site_intercept.site)

ggplot(df_site_effects, aes(B.ar1, B.site_intercept)) + geom_point() + geom_smooth()
ggplot(df_site_effects, aes(B.ar1, B.site_airTemp)) + geom_point() +geom_smooth()
ggplot(df_site_effects, aes(B.ar1, B.site_temp7p)) + geom_point() + geom_smooth()

} else {
  df_site_effects <- coef.list$B.site %>%
  dplyr::rename(B.site_intercept = B.site_intercept.site)
}


ggplot(df_site_effects, aes(B.site_airTemp, B.site_temp7p)) + geom_point() + geom_smooth()
```

No obvious trend. It's still possible, but unlikely, that AR1 term is sucking up error that would be better for predictions if included in the other random site effect coefficients.

Reran the model with AR1 fixed and no change in RMSEs

### RMSE vs. Amount of Observed Data using Trend

```{r}
obs_errors <- tempDataSyncS %>%
  group_by(featureid, year) %>%
  dplyr::mutate(resids = temp - trend) %>%
  dplyr::summarise(num_obs = n(),
                   RMSE = rmse(resids))
  
ggplot(obs_errors, aes(num_obs, RMSE)) + geom_point() + geom_smooth()

quantile(obs_errors$RMSE)

all_errors <- master_data %>%
  group_by(featureid, year, id_year) %>%
  dplyr::mutate(resids = temp - trend) %>%
  dplyr::summarise(num_obs = n(),
                   RMSE = rmse(resids))
bad_site_years <- all_errors %>%
  dplyr::filter(RMSE >= quantile(all_errors$RMSE, probs = c(0.75)))

bad_id_years <- bad_site_years$id_year

#tempDataSync <- tempDataSync %>%
 # left_join(dplyr::select(tempDataSyncS, featureid, date, fixed.ef, site.ef, huc.ef, year.ef, trend, tempPredicted, tempTrendJags))

# plot each time series
foo <- list()
bar <- list()
g <- list()
for(i in 1:length(bad_id_years)) {
  foo[[i]] <- dplyr::filter(master_data, id_year == bad_id_years[i])
  #bar[[i]] <- dplyr::filter(master_data, featureid == bad_featureid[i])
  g[[i]] <- ggplot(foo[[i]], aes(date, trend)) + geom_point(colour = "red") + geom_line(colour = "red") + geom_line(aes(date, airTemp), colour = "black") + geom_line(aes(date, temp), colour = "blue") + geom_point(aes(date, temp), colour = "blue") + ggtitle(paste0(bad_id_years[i], " | ", foo[[i]]$validation))
  print(g[[i]])
}

# check each predictor variable to see why these sites go wrong
summary(foo[[1]])
summary(bar[[1]])

# site dragging prediction down. 
dplyr::filter(df_site_effects, site == bad_featureid[1]) # no idea why site intercept would be estimated to be so low. No reason based on the data

summary(df_site_effects)

# check site & huc specific coefs for problem sites
tempDataSyncS %>% 
  group_by(featureid, huc) %>%
  dplyr::filter(featureid %in% bad_featureid) %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  dplyr::summarise_each(funs(mean))

# compare to global means
tempDataSyncS %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  colMeans()

```


### Check if the same featureid have the same problems using the JAGS trend predictions

```{r}
obs_errors_jags <- tempDataSyncS %>%
  group_by(featureid) %>%
  dplyr::mutate(resids = temp - trend) %>%
  dplyr::summarise(num_obs = n(),
                   RMSE = rmse(resids))
  
ggplot(obs_errors_jags, aes(num_obs, RMSE)) + geom_point() + geom_smooth()

mean(obs_errors_jags$RMSE)

bad_sites_jags <- obs_errors_jags %>%
  dplyr::filter(RMSE >= quantile(obs_errors_jags$RMSE, probs = c(0.99)))

bad_featureid_jags <- bad_sites_jags$featureid

# same bad sites as in R trend predictions?
bad_featureid %in% bad_featureid_jags
cbind(bad_featureid_jags, bad_featureid)

# plot each time series
foo <- list()
bar <- list()
g <- list()
for(i in 1:length(bad_featureid)) {
  foo[[i]] <- dplyr::filter(tempDataSync, featureid == bad_featureid_jags[i])
  bar[[i]] <- dplyr::filter(tempDataSyncS, featureid == bad_featureid_jags[i])
  g[[i]] <- ggplot(foo[[i]], aes(dOY, trend)) + geom_point(colour = "red") + geom_line(colour = "red") + geom_point(aes(dOY, airTemp), colour = "black") + geom_line(aes(dOY, temp), colour = "blue") + geom_point(aes(dOY, temp), colour = "blue") + ggtitle(bad_featureid_jags[i]) + facet_wrap(~year)
  print(g[[i]])
}

# check site & huc specific coefs for problem sites
tempDataSyncS %>% 
  group_by(featureid, huc) %>%
  dplyr::filter(featureid %in% bad_featureid_jags) %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  dplyr::summarise_each(funs(mean))

# compare to global means
tempDataSyncS %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  colMeans()

```


### Check if JAGS trend = Predicted trend

```{r}
ggplot(tempDataSyncS, aes(tempTrendJags, trend)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$tempTrendJags - tempDataSyncS$trend) # 0.3 seems larger than just using the mean trend from JAGS vs the predictions from the mean coefficients

tempDataSyncS$trend_diff <- tempDataSyncS$tempTrendJags - tempDataSyncS$trend

quantile(tempDataSyncS$trend_diff)

```


### Additional Validation Details

```{r}
######## Validation ##########

tempDataSyncValid <- tempDataSyncValid %>%
  left_join(dplyr::select(tempDataSyncValidS, featureid, date, site.ef, huc.ef, year.ef, trend, tempPredicted))

tempDataSyncValid <- tempDataSyncValid %>%
  dplyr::mutate(site_day = paste0(tempDataSyncValid$featureid, "_", tempDataSyncValid$date))
site_days <- unique(tempDataSync$site_day)
sites <- unique(tempDataSync$featureid)
hucs <- unique(tempDataSync$huc)
years <- unique(tempDataSync$year)
site_years <- unique(tempDataSync$featureid_year)

# overall validation
rmse(tempDataSyncValid$temp - tempDataSyncValid$trend)
rmse(tempDataSyncValid$temp - tempDataSyncValid$tempPredicted)

# missing days but sites, hucs, years with data
valid_miss_days <- tempDataSyncValid %>%
  dplyr::filter(!(site_day %in% site_days))

rmse(valid_miss_days$temp - valid_miss_days$trend)
rmse(valid_miss_days$temp - valid_miss_days$tempPredicted)

# missing sites but hucs and years with data
valid_miss_sites <- tempDataSyncValid %>%
  dplyr::filter(!(featureid %in% sites) & huc %in% hucs & year %in% years)

rmse(valid_miss_sites$temp - valid_miss_sites$trend)
rmse(valid_miss_sites$temp - valid_miss_sites$tempPredicted)

# site and huc data but missing a site_year
valid_miss_site_years <- tempDataSyncValid %>%
  dplyr::filter(!(featureid_year %in% site_years) & site %in% sites & huc %in% hucs)

rmse(valid_miss_site_years$temp - valid_miss_site_years$trend)
rmse(valid_miss_site_years$temp - valid_miss_site_years$tempPredicted)

# sites and huc data but missing years
valid_miss_years <- tempDataSyncValid %>%
  dplyr::filter(!(year %in% years) & site %in% sites & huc %in% hucs)

rmse(valid_miss_years$temp - valid_miss_years$trend)
rmse(valid_miss_years$temp - valid_miss_years$tempPredicted)

# no data - too few sites so this isn't very useful, especially without confidence intervals
valid_miss_all <- tempDataSyncValid %>%
  dplyr::filter(!(year %in% years) & !(site %in% sites) & !(huc %in% hucs))

rmse(valid_miss_all$temp - valid_miss_all$trend)
rmse(valid_miss_all$temp - valid_miss_all$tempPredicted)

# 2010
valid_2010 <- tempDataSyncValid %>%
  dplyr::filter(year == 2010)

rmse(valid_2010$temp - valid_2010$trend)
rmse(valid_2010$temp - valid_2010$tempPredicted)

# 2010 sites not used in model fitting
valid_miss_2010_sites <- tempDataSyncValid %>%
  dplyr::filter(year == 2010 & !(site %in% sites))

rmse(valid_miss_2010_sites$temp - valid_miss_2010_sites$trend)

# 2010 sites used in model fitting
valid_2010_sites <- tempDataSyncValid %>%
  dplyr::filter(year == 2010 & site %in% sites)

rmse(valid_2010_sites$temp - valid_2010_sites$trend)

```


Specifically, to evaluate the spatial and temporal predictive power of our model, we used independent validation data consisting of `r nrow(tempDataSyncValidS)` observations from `r length(unique(tempDataSyncValidS$featureid))` stream reaches representing `r length(unique(tempDataSyncValid$featureid_year))` reach-year combinations within `r length(unique(tempDataSyncValidS$huc))` HUC8 subbasins between `r min(tempDataSyncValidS$year)` - `r max(tempDataSyncValidS$year)`.


Could also look site by site depending how much huc or year data there is but there doesn't seem to be huge variation in the predictive ability with site, huc, or year data missing.

The amount of data in other years could affect the predictive ability in future years. To assess this, calculate the RMSE in each site in 2010 and the total amound of data from each of those sites that was used in the fitting. Then do a simple regression and plot.

```{r validation data amounts}
rmse_2010 <- tempDataSyncValid %>%
  dplyr::filter(year == 2010) %>%
  dplyr::group_by(featureid) %>%
  dplyr::summarise(obs_2010 = n(),
                   rmse = rmse(temp - trend))

fit_sum <- tempDataSync %>%
  dplyr::group_by(featureid) %>%
  dplyr::summarise(obs = n(), 
                   years = length(unique(year)))

rmse_2010 <- rmse_2010 %>%
  dplyr::left_join(fit_sum) %>%
  replace(is.na(.), 0)
rmse_2010
summary(rmse_2010)

# 95% confidence intervals
rmse_bs <- sample(rmse_2010$rmse, 10000, replace = TRUE)
quantile(rmse_bs, c(0.025, 0.975))

g <- ggplot(rmse_2010, aes(obs, rmse)) + 
  geom_point(aes(colour = years)) + 
  geom_smooth(level = 0.95) + 
  scale_color_gradient(low = "black", high = "red") + 
  #xlim(0, 5) +
  #ylim(0, 5) +
  xlab("Number of observations (days with data) used to fit the model") +
  ylab("RMSE by reach in 2010") +
  theme_bw_journal()

g
ggsave(file.path("manuscripts/Figures/rmse_2010_obs_plot.jpg"), plot = g, device = "jpg")

# RMSE based on amount of data in the validation set
g <- ggplot(rmse_2010, aes(obs_2010, rmse)) + 
  geom_point(aes(colour = obs)) + 
  geom_smooth(level = 0.95) + 
  scale_color_gradient(low = "black", high = "red") + 
  #xlim(0, 5) +
  #ylim(0, 5) +
  xlab("Number of observations in the validation data per reach") +
  ylab("RMSE by reach in 2010") +
  theme_bw_journal()

g
ggsave(file.path("manuscripts/Figures/rmse_2010_valid_obs_plot.jpg"), plot = g, device = "jpg")
```

Plot all the validation data vs the observed to see if any bias

```{r plot all validation data vs observed}
g <- ggplot(tempDataSyncValid, aes(temp, trend)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  xlim(0, 30) +
  ylim(0, 30) +
  xlab("Observed Water Temperature (C)") +
  ylab("Predicted Water Temperature (C)") +
  theme_bw_journal()

g
ggsave(file.path("manuscripts/Figures/validation_plot.jpg"), plot = g, device = "jpg")
```

Amount of data used in fitting effect on validation RMSE

```{r validation data amounts}
rmse_obs <- tempDataSyncValid %>%
  dplyr::group_by(featureid) %>%
  dplyr::summarise(rmse = rmse(temp - trend))

rmse_obs <- rmse_obs %>%
  dplyr::left_join(fit_sum) %>%
  replace(is.na(.), 0)
rmse_obs
summary(rmse_obs)

# 95% confidence intervals
rmse_bs <- sample(rmse_obs$rmse, 10000, replace = TRUE)
quantile(rmse_bs, c(0.025, 0.975))

g <- ggplot(rmse_obs, aes(obs, rmse)) + 
  geom_point(aes(colour = years)) + 
  geom_smooth(level = 0.95) + 
  scale_color_gradient(low = "black", high = "red") + 
  #xlim(0, 5) +
  #ylim(0, 5) +
  xlab("Number of observations (days with data) used to fit the model") +
  ylab("RMSE by reach") +
  theme_bw_journal()

g
ggsave(file.path("manuscripts/Figures/rmse_obs_plot.jpg"), plot = g, device = "jpg")
```

### Evaluation of breakpoints

```{r}


```


### Derived metrics

```{r}
summary(derived.metrics) # includes VA to ME

derived.metrics <- derived.metrics %>%
  dplyr::filter(!is.na(AreaSqKM))

summary(derived.metrics)
```



### ERROR summary by site 

```{r}
tempDataSyncValid <- tempDataSyncValid %>%
  group_by(featureid) %>%
  filter(!(is.na(temp) & !is.na(trend))) %>%
  mutate(error = temp - trend)

foo <- tempDataSyncValid %>%
  dplyr::mutate(impoundArea = AreaSqKM * allonnet / 100) %>%
  dplyr::summarise_each(. , funs(mean)) 

valid_summary <- tempDataSyncValid %>%
  group_by(featureid) %>%
  dplyr::summarise(RMSE = rmse(error),
                MAE = mae(error),
                NSE = nse(temp, trend)) %>%
  dplyr::left_join(foo)

valid_summary %>%
  dplyr::select(temp, forest, AreaSqKM, impoundArea, developed, elevation, impervious, allonnet, RMSE, MAE, NSE) %>%
  summary()

ggplot(valid_summary, aes(RMSE, MAE)) + geom_point()
ggplot(valid_summary, aes(RMSE, NSE)) + geom_point()
```

### characteristics of sites correlated with predictive ability?

```{r}
g <- ggplot(valid_summary, aes(forest, RMSE)) +
  geom_point() +
  geom_smooth() +
  xlab("Forest Cover (%)")
  theme_bw_journal()
g
ggsave("manuscripts/Figures/forest_rmse.jpg", g, device = "jpg")

g <- ggplot(valid_summary, aes(AreaSqKM, RMSE)) +
  geom_point() +
  geom_smooth() +
  xlab("Drainage Area (sq KM)")
  theme_bw_journal()
g
ggsave("manuscripts/Figures/drainage_rmse.jpg", g, device = "jpg")

g <- ggplot(valid_summary, aes(allonnet, RMSE)) +
  geom_point() +
  geom_smooth() +
  xlab("Percentage of drainage area impounded")
  theme_bw_journal()
g
ggsave("manuscripts/Figures/allonnet_rmse.jpg", g, device = "jpg")

g <- ggplot(valid_summary, aes(developed, RMSE)) +
  geom_point() +
  geom_smooth() +
  xlab("Percentage of catchment developed")
  theme_bw_journal()
g
ggsave("manuscripts/Figures/developed_rmse.jpg", g, device = "jpg")

library(GGally)
short_sum <- valid_summary %>%
  dplyr::select(temp, forest, AreaSqKM, impoundArea, developed, elevation, impervious, allonnet, RMSE, NSE)
ggpairs(short_sum)

valid_summary_year <- tempDataSyncValid %>%
  group_by(featureid, year) %>%
  dplyr::summarise(RMSE = rmse(error),
                MAE = mae(error),
                NSE = nse(temp, trend))

g <- ggplot(valid_summary_year, aes(as.character(year), RMSE)) +
  geom_boxplot() +
  xlab("Year") +
  theme_bw_journal()
g
ggsave("manuscripts/Figures/year_rmse.jpg", g, device = "jpg")
```


### characteristics of sites with good vs. poor predictions

RMSE and other metrics currently include AR1 adjustment - redo with both trend and adjusted

```{r}
df_poor <- error_metrics_year %>%
  dplyr::filter(RMSE_trend > quantile(error_metrics_year$RMSE_trend, probs = c(0.9), na.rm = TRUE))
summary(df_poor)
```

No obvious landscape characteristics causing poor predictions


## map rmse


### Derived Metrics of Observations

Predictions can be off by quite a bit (2+ C) on any given day but how bad is RMSE for derived metrics like meanJulyTemp?

```{r}
summary(derived.metrics.obs)

rmse(derived.metrics.obs$meanJulyObs - derived.metrics.obs$meanJulyTemp)

ggplot(derived.metrics.obs, aes(meanJulyObs, meanJulyTemp)) + geom_point() + geom_abline(intercept = 0, slope = 1)
```

Just as bad in July as across all predictions or even worse.














