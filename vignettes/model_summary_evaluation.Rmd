---
title: "Model Summary and Validation"
author: "Daniel J Hocking"
date: "11/9/2015"
output: html_document
---


Load packages

```{r}
library(dplyr)
library(conteStreamTemperature)
```

load data

```{r}
data_dir <- "localData_2015-11-20" 

load(paste0(data_dir, "/tempDataSync.RData"))

M.ar1 <- readRDS(paste0(data_dir, "/jags.RData"))

cov.list <- readRDS(paste0(data_dir, "/covariate_list.RData"))

coef.list <- readRDS(paste0(data_dir, "/coef.RData"))

derived.metrics <- readRDS(paste0(data_dir, "/derived_site_metrics.RData"))

breakpoints <- readRDS(paste0(data_dir, "/springFallBPs.RData"))

derived.metrics.obs <- readRDS(paste0(data_dir, "/derived_site_metrics_observed.RData"))

```


### Coefficients

Summarize coefficients and make sure they're reasonable

```{r}
format(coef.list$fix.ef, digits = 2)

format(coef.list$mu.huc, digits = 2)

format(coef.list$sigma.site, digits = 2)

format(coef.list$sigma.huc, digits = 2)

format(coef.list$sigma.year, digits = 2)

format(coef.list$sigma.ar1, digits = 2)

format(coef.list$cor.huc, digits = 2)

format(coef.list$cor.year, digits = 2)

```

Surprisingly no effect of forest cover, but it might get sucked up by the previous 7-day air temperature. It's also possible that there just isn't sufficient power to have forest, agriculture, and hi-development plus interactions and all the random effects in the model.

There is large variablity among sites and hucs. Little variation of AR1 among sites.


### Check RMSE predictions in R

```{r}
# Predictions in R
tempDataSyncS <- predictTemp(fullDataSyncS = tempDataSyncS, coef.list = coef.list, rand_ids = rand_ids, Random_AR1 = FALSE)

rmse(tempDataSyncS$temp - tempDataSyncS$tempPredicted) # 0.6398
  
```

### Check predictions in JAGS

```{r}
df.ar1 <- as.data.frame(as.matrix(M.ar1)) # massive

# predictions from JAGS
temp.predicted.mean <- df.ar1 %>%
  dplyr::select(starts_with("stream.mu")) %>%
  colMeans()
tempDataSyncS$tempPredictedJags <- as.numeric(temp.predicted.mean)

ggplot(tempDataSyncS, aes(temp, tempPredictedJags)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$temp - tempDataSyncS$tempPredictedJags) # 0.633 - slightly diff than R predictions

```


### JAGS trend predictions

```{r}
# predictions from JAGS
temp.trend.mean <- df.ar1 %>%
  dplyr::select(starts_with("trend")) %>%
  colMeans()
tempDataSyncS$tempTrendJags <- NA_real_
tempDataSyncS[1:length(temp.trend.mean), ]$tempTrendJags <- as.numeric(temp.trend.mean)

ggplot(tempDataSyncS, aes(temp, tempTrendJags)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$temp - tempDataSyncS$tempTrendJags) # 1.15
rmse(tempDataSyncS$temp - tempDataSyncS$trend) # 1.19

```

### Check That Prediction Function Matches JAGS Predictions

```{r}
ggplot(tempDataSyncS, aes(tempPredictedJags, tempPredicted)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$tempPredictedJags - tempDataSyncS$tempPredicted)
```

The predictions are slightly different when calculated in JAGS vs. in R. I think the difference is in calculating them for each iteration then taking the mean (in JAGS) compared with taking the mean coefficients then doing the predictions. 


### Test Predictions and RMSE for observed and validation data

```{r}
rmse(tempDataSyncS$temp - tempDataSyncS$tempPredicted)
rmse(tempDataSyncS$temp - tempDataSyncS$tempPredictedJags)
rmse(tempDataSyncS$temp - tempDataSyncS$trend) 

tempDataSyncValidS <- predictTemp(fullDataSyncS = tempDataSyncValidS, coef.list = coef.list, rand_ids = rand_ids, Random_AR1 = FALSE)
rmse(tempDataSyncValidS$temp - tempDataSyncValidS$tempPredicted) 
rmse(tempDataSyncValidS$temp - tempDataSyncValidS$trend) 
```

Very disappointing validation. Potential problems:

* difficult validation holding out sites, hucs, and years
* Correlation of airTemp and 7-day air temp with the cubic day of the year
* don't adequately address things like groundwater, dams, and tidal influence
* problem with one or more of the functions
* Imprecise covariates
* Not adequate QAQC on temperature data
* model is overfitted, especially with all the random site, huc, and year effects. Overfitted models do well with fitting the data but poorly at predicting new data.


### Plot observed, predicted, and air temperatures

```{r}
foo <- dplyr::select(tempDataSyncS, featureid, date, fixed.ef, site.ef, huc.ef, year.ef, trend, tempPredicted, tempPredictedJags, tempTrendJags)
tempDataSync <- tempDataSync %>%
  dplyr::left_join(foo)

foo <- dplyr::select(tempDataSyncValidS, featureid, date, fixed.ef, site.ef, huc.ef, year.ef, trend, tempPredicted)
tempDataSyncValid <- tempDataSyncValid %>%
  dplyr::left_join(foo)

master_data <- dplyr::rbind_list(tempDataSync, tempDataSyncValid)

master_data <- master_data %>%
  flag_daily_rise(.)

# Median Absolute Deviations for spike detection
master_data <- master_data %>%
  dplyr::group_by(featureid, year) %>%
  dplyr::mutate(MAD_normalized = MAD.roller(temp, 6),
                MAD_normalized = MAD.roller(temp, 10),
                MAD_normalized_30 = MAD.roller(temp, 30),
                flag_MAD = ifelse(MAD_normalized > 10, TRUE, FALSE)) # 10 day moving window

master_data$id_year <- paste0(master_data$featureid, "_", master_data$year)

series <- unique(master_data$id_year)
n <- length(series)
for(i in 1:n) {
  data_1 <- dplyr::filter(master_data, id_year == series[i])
  g <- ggplot(data_1, aes(date, temp)) + ylim(c(0, 35)) + ggtitle(paste0("featureid: ", data_1$featureid[1], "  year: ", data_1$year[1])) #+ facet_wrap(~year)
#   if(nrow(bad_MAD_30) > 0) {
#     g <- g + geom_point(data = bad_MAD_30, aes(datetime, temp), colour = "yellow", size = 5, alpha = 0.5)
#   }
  g <- g + geom_point(aes(colour = MAD_normalized)) + geom_line(aes(colour = MAD_normalized)) + 
    scale_colour_gradient(high = "yellow", low = "blue", limits = c(0, 6)) 
  
  # add agency name
  g <- g +
    geom_line(aes(date, airTemp), colour = "black") +
    #geom_line(aes(date, tmax), colour = "gray") +
    #geom_line(aes(date, tmin), colour = "gray") +
    geom_line(aes(date, trend), colour = "red") + 
    xlab("Date") + 
    ylab("Temperature (C)") +
    theme_bw()
  ggsave(paste0(data_dir, "/diagnostics/obs_pred_ts_", series[i], ".png"), plot = g)
}



# output daily flags
d_flags <- df_values3 %>%
  dplyr::filter(flag_daily_rise == TRUE |
                  flag_cold_days == TRUE |
                  flag_hot_days == TRUE |
                  flag_extreme_days == TRUE) %>%
  dplyr::select(-row, -temp_prev, -series_prev, -series_start, -median_freq, -min_n90)

# Median Absolute Deviations for spike detection



```




### Is there a trade-off with AR1 term by site and other random site effects?

```{r, echo=FALSE}
if(Random_AR1) {
df_site_effects <- coef.list$B.site %>%
  dplyr::left_join(dplyr::select(coef.list$B.ar1, sitef, site, B.ar1 = mean)) %>%
  dplyr::rename(B.site_intercept = B.site_intercept.site)
} else {
  df_site_effects <- coef.list$B.site %>%
  dplyr::left_join(dplyr::select(coef.list$B.ar1, sitef, site, B.ar1 = mean)) %>%
  dplyr::rename(B.site_intercept = B.site_intercept.site)
}

ggplot(df_site_effects, aes(B.ar1, B.site_intercept)) + geom_point() + geom_smooth()
ggplot(df_site_effects, aes(B.ar1, B.site_airTemp)) + geom_point() +geom_smooth()
ggplot(df_site_effects, aes(B.ar1, B.site_temp7p)) + geom_point() + geom_smooth()

ggplot(df_site_effects, aes(B.site_airTemp, B.site_temp7p)) + geom_point() + geom_smooth()
```

No obvious trend. It's still possible, but unlikely, that AR1 term is sucking up error that would be better for predictions if included in the other random site effect coefficients.

Reran the model with AR1 fixed and no change in RMSEs

### RMSE vs. Amount of Observed Data using Trend

```{r}
obs_errors <- tempDataSyncS %>%
  group_by(featureid) %>%
  dplyr::mutate(resids = temp - trend) %>%
  dplyr::summarise(num_obs = n(),
                   RMSE = rmse(resids))
  
ggplot(obs_errors, aes(num_obs, RMSE)) + geom_point() + geom_smooth()

quantile(obs_errors$RMSE)

bad_sites <- obs_errors %>%
  dplyr::filter(RMSE >= quantile(obs_errors$RMSE, probs = c(0.99)))

bad_featureid <- bad_sites$featureid

tempDataSync <- tempDataSync %>%
  left_join(dplyr::select(tempDataSyncS, featureid, date, fixed.ef, site.ef, huc.ef, year.ef, trend, tempPredicted, tempTrendJags))

# plot each time series
foo <- list()
bar <- list()
g <- list()
for(i in 1:length(bad_featureid)) {
  foo[[i]] <- dplyr::filter(tempDataSync, featureid == bad_featureid[i])
  bar[[i]] <- dplyr::filter(tempDataSyncS, featureid == bad_featureid[i])
  g[[i]] <- ggplot(foo[[i]], aes(date, trend)) + geom_point(colour = "red") + geom_line(colour = "red") + geom_point(aes(date, airTemp), colour = "black") + geom_line(aes(date, temp), colour = "blue") + geom_point(aes(date, temp), colour = "blue") + ggtitle(bad_featureid[i])
  print(g[[i]])
}

# check each predictor variable to see why these sites go wrong
summary(foo[[1]])
summary(bar[[1]])

# site dragging prediction down. 
dplyr::filter(df_site_effects, site == bad_featureid[1]) # no idea why site intercept would be estimated to be so low. No reason based on the data

summary(df_site_effects)

# check site & huc specific coefs for problem sites
tempDataSyncS %>% 
  group_by(featureid, huc) %>%
  dplyr::filter(featureid %in% bad_featureid) %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  dplyr::summarise_each(funs(mean))

# compare to global means
tempDataSyncS %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  colMeans()

```


### Check if the same featureid have the same problems using the JAGS trend predictions

```{r}
obs_errors_jags <- tempDataSyncS %>%
  group_by(featureid) %>%
  dplyr::mutate(resids = temp - trend) %>%
  dplyr::summarise(num_obs = n(),
                   RMSE = rmse(resids))
  
ggplot(obs_errors_jags, aes(num_obs, RMSE)) + geom_point() + geom_smooth()

mean(obs_errors_jags$RMSE)

bad_sites_jags <- obs_errors_jags %>%
  dplyr::filter(RMSE >= quantile(obs_errors_jags$RMSE, probs = c(0.99)))

bad_featureid_jags <- bad_sites_jags$featureid

# same bad sites as in R trend predictions?
bad_featureid %in% bad_featureid_jags
cbind(bad_featureid_jags, bad_featureid)

# plot each time series
foo <- list()
bar <- list()
g <- list()
for(i in 1:length(bad_featureid)) {
  foo[[i]] <- dplyr::filter(tempDataSync, featureid == bad_featureid_jags[i])
  bar[[i]] <- dplyr::filter(tempDataSyncS, featureid == bad_featureid_jags[i])
  g[[i]] <- ggplot(foo[[i]], aes(dOY, trend)) + geom_point(colour = "red") + geom_line(colour = "red") + geom_point(aes(dOY, airTemp), colour = "black") + geom_line(aes(dOY, temp), colour = "blue") + geom_point(aes(dOY, temp), colour = "blue") + ggtitle(bad_featureid_jags[i]) + facet_wrap(~year)
  print(g[[i]])
}

# check site & huc specific coefs for problem sites
tempDataSyncS %>% 
  group_by(featureid, huc) %>%
  dplyr::filter(featureid %in% bad_featureid_jags) %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  dplyr::summarise_each(funs(mean))

# compare to global means
tempDataSyncS %>%
  dplyr::select(starts_with("B.site"), starts_with("B.huc")) %>%
  colMeans()

```


### Check if JAGS trend = Predicted trend

```{r}
ggplot(tempDataSyncS, aes(tempTrendJags, trend)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = "blue")

rmse(tempDataSyncS$tempTrendJags - tempDataSyncS$trend) # 0.3 seems larger than just using the mean trend from JAGS vs the predictions from the mean coefficients

tempDataSyncS$trend_diff <- tempDataSyncS$tempTrendJags - tempDataSyncS$trend

quantile(tempDataSyncS$trend_diff)

```


### Additional Validation Details

```{r}
######## Validation ##########

tempDataSyncValid <- tempDataSyncValid %>%
  left_join(dplyr::select(tempDataSyncValidS, featureid, date, site.ef, huc.ef, year.ef, trend, tempPredicted))

tempDataSyncValid <- tempDataSyncValid %>%
  dplyr::mutate(site_day = paste0(tempDataSyncValid$featureid, "_", tempDataSyncValid$date))
site_days <- unique(tempDataSync$site_day)
sites <- unique(tempDataSync$featureid)
hucs <- unique(tempDataSync$huc)
years <- unique(tempDataSync$year)

# overall validation
rmse(tempDataSyncValid$temp - tempDataSyncValid$trend)
rmse(tempDataSyncValid$temp - tempDataSyncValid$tempPredicted)

# missing days but sites, hucs, years with data
valid_miss_days <- tempDataSyncValid %>%
  dplyr::filter(!(site_day %in% site_days))

rmse(valid_miss_days$temp - valid_miss_days$trend)
rmse(valid_miss_days$temp - valid_miss_days$tempPredicted)

# missing sites but hucs and years with data
valid_miss_sites <- tempDataSyncValid %>%
  dplyr::filter(!(featureid %in% sites) & huc %in% hucs & year %in% years)

rmse(valid_miss_sites$temp - valid_miss_sites$trend)
rmse(valid_miss_sites$temp - valid_miss_sites$tempPredicted)

# sites and huc data but missing years
valid_miss_years <- tempDataSyncValid %>%
  dplyr::filter(!(year %in% years) & site %in% sites & huc %in% hucs)

rmse(valid_miss_years$temp - valid_miss_years$trend)
rmse(valid_miss_years$temp - valid_miss_years$tempPredicted)

# no data
valid_miss_all <- tempDataSyncValid %>%
  dplyr::filter(!(year %in% years) & !(site %in% sites) & !(huc %in% hucs))

rmse(valid_miss_all$temp - valid_miss_all$trend)
rmse(valid_miss_all$temp - valid_miss_all$tempPredicted)
```

Could also look site by site depending how much huc or year data there is but there doesn't seem to be huge variation in the predictive ability with site, huc, or year data missing.


### Evaluation of breakpoints

```{r}


```


### Derived metrics

```{r}
summary(derived.metrics) # includes VA to ME

derived.metrics <- derived.metrics %>%
  dplyr::filter(!is.na(AreaSqKM))

summary(derived.metrics)
```



### ERROR summary by site and year

```{r}
tempDataSyncS_resids <- tempDataSyncS %>%
  group_by(featureid, year) %>%
  filter(!(is.na(temp) & !is.na(trend))) %>%
  mutate(error = temp - trend)

if(dim(tempDataSyncS_resids)[1] > 0) {
  error_metrics_year <- tempDataSyncS_resids %>%
    dplyr::summarise(RMSE_trend = rmse(error),
                     MAE_trend = mae(error),
                     NSE_trend = nse(temp, tempPredicted)) 
  error_metrics <- error_metrics_year %>%
    dplyr::summarise(meanRMSE_trend = mean(RMSE_trend, na.rm = T),
                     meanMAE_trend = mean(MAE_trend, na.rm = T),
                     meanNSE_trend = mean(NSE_trend, na.rm = T))

  error_metrics_year <- error_metrics_year %>%
    left_join(df_covariates_upstream) %>%
    dplyr::mutate(impoundArea = AreaSqKM * allonnet / 100)
}

summary(error_metrics_year)

```

### characteristics of sites correlated with predictive ability?

```{r}
ggplot(error_metrics_year, aes(AreaSqKM, RMSE_trend)) + geom_point() + geom_smooth()

ggplot(error_metrics_year, aes(impoundArea, RMSE_trend)) + geom_point() + geom_smooth()

ggplot(error_metrics_year, aes(forest, RMSE_trend)) + geom_point() + geom_smooth()

ggplot(error_metrics_year, aes(year, RMSE_trend)) + geom_point() + geom_smooth()

```


### characteristics of sites with good vs. poor predictions

RMSE and other metrics currently include AR1 adjustment - redo with both trend and adjusted

```{r}
df_poor <- error_metrics_year %>%
  dplyr::filter(RMSE_trend > quantile(error_metrics_year$RMSE_trend, probs = c(0.9), na.rm = TRUE))
summary(df_poor)
```

No obvious landscape characteristics causing poor predictions


## map rmse


### Derived Metrics of Observations

Predictions can be off by quite a bit (2+ C) on any given day but how bad is RMSE for derived metrics like meanJulyTemp?

```{r}
summary(derived.metrics.obs)

rmse(derived.metrics.obs$meanJulyObs - derived.metrics.obs$meanJulyTemp)

ggplot(derived.metrics.obs, aes(meanJulyObs, meanJulyTemp)) + geom_point() + geom_abline(intercept = 0, slope = 1)
```

Just as bad in July as across all predictions or even worse.














